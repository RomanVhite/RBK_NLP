{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "stanza.download(\"ru\")\n",
    "\n",
    "nlp = spacy_stanza.load_pipeline(\"ru\")\n",
    "\n",
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание стартовых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"full_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"full_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train_df[[\"title\",\"category\",\"authors\", \"tags\", \"publish_date\"]].append(test_df[[\"title\",\"category\",\"authors\", \"tags\", \"publish_date\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols[\"category\"] = cat_cols[\"category\"].astype('category')\n",
    "cat_cols[\"category\"] = cat_cols[\"category\"].cat.codes\n",
    "cat_cols[\"category\"] = cat_cols[\"category\"].astype('int')\n",
    "cat_cols[\"authors\"] = cat_cols[\"authors\"].astype('category')\n",
    "cat_cols[\"authors\"] = cat_cols[\"authors\"].cat.codes\n",
    "cat_cols[\"authors\"] = cat_cols[\"authors\"].astype('int')\n",
    "cat_cols[\"tags\"] = cat_cols[\"tags\"].astype('category')\n",
    "cat_cols[\"tags\"] = cat_cols[\"tags\"].cat.codes\n",
    "cat_cols[\"tags\"] = cat_cols[\"tags\"].astype('int')\n",
    "cat_cols['day'] = pd.to_datetime(cat_cols['publish_date']).dt.strftime(\"%d\").astype(int)\n",
    "cat_cols['mounth'] = pd.to_datetime(cat_cols['publish_date']).dt.strftime(\"%m\").astype(int)\n",
    "cat_cols['hour'] = pd.to_datetime(cat_cols['publish_date']).dt.strftime(\"%H\").astype(int)\n",
    "cat_cols['minute'] = pd.to_datetime(cat_cols['publish_date']).dt.strftime(\"%M\").astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начало и конец месяца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = []\n",
    "end = []\n",
    "for i, day in enumerate(cat_cols[\"day\"]):\n",
    "    #print(day)\n",
    "    if day < 5:\n",
    "        start.append(1.0)\n",
    "    else:\n",
    "        start.append(0.0)\n",
    "    if day > 27:\n",
    "        end.append(1.0)\n",
    "    else:\n",
    "        end.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols[\"month_end\"] = end\n",
    "cat_cols[\"month_start\"] = start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Время публикации новости: прайм тайм (после 19:00) и начало дня(до 11:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime = []\n",
    "morning = []\n",
    "for i, hour in enumerate(cat_cols[\"hour\"]):\n",
    "    #print(day)\n",
    "    if hour > 19:\n",
    "        prime.append(1.0)\n",
    "    else:\n",
    "        prime.append(0.0)\n",
    "    if hour < 11:\n",
    "        morning.append(1.0)\n",
    "    else:\n",
    "        morning.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols[\"prime_time\"] = prime\n",
    "cat_cols[\"morning\"] = morning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = cat_cols[\"title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,re\n",
    "from tqdm import tqdm\n",
    "def text_cleaner(list_corpus):\n",
    "    results = []\n",
    "    for sentence in tqdm(list_corpus):\n",
    "        #regex = re.compile('\\s+[\\w]+,\\s[\\d]+.[\\d]+') \\s{5}.+\n",
    "        regex = re.compile('\\s{5}.+')\n",
    "        sentence = regex.sub('', sentence).translate(string.punctuation)\n",
    "        results.append(sentence)\n",
    "    return results\n",
    "clean_list_corpus = text_cleaner(list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        if (word not in stopwords and len(word) > 1):\n",
    "            p = morph.parse(word)[0]\n",
    "            res.append(p.normal_form)\n",
    "    text = \" \".join(res)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_corpus = [lemmatize(text) for text in clean_list_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_df = pd.DataFrame({\"lem_title\": lemmatized_corpus}, index=cat_cols.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = pd.merge(cat_cols, lem_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохранение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop([\"title\",\"authors\",\"category\",\"authors\", \"tags\", \"publish_date\"], axis=1)\n",
    "test_df = test_df.drop([\"title\",\"authors\",\"category\",\"authors\", \"tags\", \"publish_date\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.merge(train_df, cat_cols, left_index=True, right_index=True)\n",
    "new_test_df = pd.merge(test_df, cat_cols, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df.to_csv(\"new_test.csv\")\n",
    "new_train_df.to_csv(\"new_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование текстовых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"new_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "test_df_to_append = pd.read_csv(\"new_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_dataset_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_dataset_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Векторизация текста статьи с помощью doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lem = train_df[[\"article\",\"session\"]].append(test_df[[\"article\",\"session\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_lem[\"article\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "train_set = []\n",
    "for i, sent in tqdm(enumerate(cleaned_corpus)):\n",
    "    #print(type(sent),i)\n",
    "    nltk_tokens = nltk.word_tokenize(sent)\n",
    "    train_set.append(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_set)]\n",
    "model = Doc2Vec(documents, vector_size=300, window=4, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_corpus = []\n",
    "for i, sents in tqdm(enumerate(train_set)):\n",
    "    transformed_corpus.append(model.infer_vector(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_cols = [str(i) for i in list(np.arange(0,300))]\n",
    "emb_names=[\"doc2vec_article_\"+str(i) for i in embed_cols]\n",
    "\n",
    "doc2vec_encoded = pd.DataFrame(transformed_corpus, columns=emb_names)\n",
    "doc2vec_encoded.index = corpus_lem.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, doc2vec_encoded, left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, doc2vec_encoded, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ тональности текста статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_df[\"article\"].append(test_df[\"article\"])\n",
    "list_corpus = corpus.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(list_corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitive_analysis(cleaned_corpus):\n",
    "    tokenizer = RegexTokenizer()\n",
    "    model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
    "    results = model.predict(cleaned_corpus, k=5)\n",
    "    return results\n",
    "s_a = sensitive_analysis(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_s_an = pd.DataFrame(s_a, index=corpus.index)\n",
    "after_s_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, after_s_an, left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, after_s_an, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кодирование признака keyfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lem = train_df[[\"keyfeatures\",\"session\"]].append(test_df[[\"keyfeatures\",\"session\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_lem[\"keyfeatures\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    text= text.lower()\n",
    "    cleaned_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_vect = CountVectorizer(max_features=500)\n",
    "kf_transformed = kf_vect.fit_transform(cleaned_corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = kf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(kf_transformed)\n",
    "df.columns = kf_vect.get_feature_names()\n",
    "df.index = corpus_lem.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, df, left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кодирование авторов с помощью метода CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lem = train_df[[\"new_authors\",\"session\"]].append(test_df[[\"new_authors\",\"session\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_lem[\"new_authors\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    text= text.lower()\n",
    "    cleaned_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "from tqdm import tqdm\n",
    "for i, elem in tqdm(enumerate(cleaned_corpus)):\n",
    "    if len(elem) ==3:\n",
    "        #print(\"rbk\")\n",
    "        new_corpus.append(elem)\n",
    "    else:\n",
    "        elem_tokens = nltk.word_tokenize(elem)\n",
    "        #print(elem_tokens)\n",
    "        name = \"\"\n",
    "        surname = \"\"\n",
    "        if len(elem_tokens) > 2:\n",
    "            new_authors = \"\"\n",
    "            for j, token in enumerate(elem_tokens):\n",
    "                if j%2==0:\n",
    "                    name = token\n",
    "                elif j%2!=0:\n",
    "                    surname = token\n",
    "                if name !=\"\" and surname!=\"\":\n",
    "                    new_authors+=name+\"_\"+surname+\" \"\n",
    "                    name = \"\"\n",
    "                    surname = \"\"\n",
    "            new_corpus.append(new_authors)\n",
    "        else:\n",
    "            new_corpus.append(elem_tokens[0]+\"_\"+elem_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_vect = CountVectorizer(binary=True,analyzer='word')\n",
    "af_transformed = af_vect.fit_transform(new_corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(af_transformed)\n",
    "df.columns = af_vect.get_feature_names()\n",
    "df.index = corpus_lem.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "names= af_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, df, left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for index, row in tqdm(train_df_to_append.iterrows()):\n",
    "    train_df_to_append.loc[index, 'article_len'] = int(len(row.article))\n",
    "lem_len = []\n",
    "\n",
    "for index, row in tqdm(train_df_to_append.iterrows()):\n",
    "    train_df_to_append.loc[index, 'title_len'] = int(len(row.lem_title)) \n",
    "    \n",
    "for index, row in tqdm(train_df_to_append.iterrows()):\n",
    "    train_df_to_append.loc[index, 'keyfeatures_len'] = int(len(str(row.keyfeatures))) \n",
    "\n",
    "import re\n",
    "corpus = train_df_to_append.article.tolist()\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)\n",
    "train_set = []\n",
    "for i, sent in tqdm(enumerate(cleaned_corpus)):\n",
    "    #print(type(sent),i)\n",
    "    nltk_tokens = nltk.word_tokenize(sent)\n",
    "    train_set.append(nltk_tokens)\n",
    "it = 0\n",
    "for index, row in tqdm(train_df_to_append.iterrows()):\n",
    "    train_df_to_append.loc[index, 'article_word_count'] = int(len(train_set[it]))\n",
    "    it+=1\n",
    "    \n",
    "corpus = train_df_to_append.lem_title.tolist()\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)\n",
    "train_set = []\n",
    "for i, sent in tqdm(enumerate(cleaned_corpus)):\n",
    "    #print(type(sent),i)\n",
    "    nltk_tokens = nltk.word_tokenize(sent)\n",
    "    train_set.append(nltk_tokens)\n",
    "it = 0\n",
    "for index, row in tqdm(train_df_to_append.iterrows()):\n",
    "    train_df_to_append.loc[index, 'title_word_count'] = int(len(train_set[it]))\n",
    "    it+=1\n",
    "    \n",
    "corpus = train_df_to_append.keyfeatures.tolist()\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', str(sentence))\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)\n",
    "train_set = []\n",
    "for i, sent in tqdm(enumerate(cleaned_corpus)):\n",
    "    #print(type(sent),i)\n",
    "    nltk_tokens = nltk.word_tokenize(str(sent))\n",
    "    train_set.append(nltk_tokens)\n",
    "it = 0\n",
    "for index, row in tqdm(train_df_to_append.iterrows()):\n",
    "    train_df_to_append.loc[index, 'keyfeatures_word_count'] = int(len(train_set[it]))\n",
    "    it+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for index, row in tqdm(test_df_to_append.iterrows()):\n",
    "    test_df_to_append.loc[index, 'article_len'] = int(len(row.article))\n",
    "lem_len = []\n",
    "\n",
    "for index, row in tqdm(test_df_to_append.iterrows()):\n",
    "    test_df_to_append.loc[index, 'title_len'] = int(len(row.lem_title)) \n",
    "    \n",
    "for index, row in tqdm(test_df_to_append.iterrows()):\n",
    "    test_df_to_append.loc[index, 'keyfeatures_len'] = int(len(str(row.keyfeatures))) \n",
    "\n",
    "import re\n",
    "corpus = test_df_to_append.article.tolist()\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)\n",
    "train_set = []\n",
    "for i, sent in tqdm(enumerate(cleaned_corpus)):\n",
    "    #print(type(sent),i)\n",
    "    nltk_tokens = nltk.word_tokenize(sent)\n",
    "    train_set.append(nltk_tokens)\n",
    "it = 0\n",
    "for index, row in tqdm(test_df_to_append.iterrows()):\n",
    "    test_df_to_append.loc[index, 'article_word_count'] = int(len(train_set[it]))\n",
    "    it+=1\n",
    "    \n",
    "corpus = test_df_to_append.lem_title.tolist()\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)\n",
    "train_set = []\n",
    "for i, sent in tqdm(enumerate(cleaned_corpus)):\n",
    "    #print(type(sent),i)\n",
    "    nltk_tokens = nltk.word_tokenize(sent)\n",
    "    train_set.append(nltk_tokens)\n",
    "it = 0\n",
    "for index, row in tqdm(test_df_to_append.iterrows()):\n",
    "    test_df_to_append.loc[index, 'title_word_count'] = int(len(train_set[it]))\n",
    "    it+=1\n",
    "    \n",
    "corpus = test_df_to_append.keyfeatures.tolist()\n",
    "cleaned_corpus = []\n",
    "for i, sentence in enumerate(corpus):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', str(sentence))\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«,.»$]\", \" \", rem_num)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    cleaned_corpus.append(text)\n",
    "train_set = []\n",
    "for i, sent in tqdm(enumerate(cleaned_corpus)):\n",
    "    #print(type(sent),i)\n",
    "    nltk_tokens = nltk.word_tokenize(str(sent))\n",
    "    train_set.append(nltk_tokens)\n",
    "it = 0\n",
    "for index, row in tqdm(test_df_to_append.iterrows()):\n",
    "    test_df_to_append.loc[index, 'keyfeatures_word_count'] = int(len(train_set[it]))\n",
    "    it+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append.to_csv(\"cleaned_train.csv\")\n",
    "test_df_to_append.to_csv(\"cleaned_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
