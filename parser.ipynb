{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсер данных с сайта РБК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import time\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок парсинга текста статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_dataset_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_dataset_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag = train_df[[\"title\",\"session\"]].append(test_df[[\"title\",\"session\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция для удаления html тэгов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data): \n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция для выполнения лемматизации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        if (word not in stopwords and len(word) > 1):\n",
    "            p = morph.parse(word)[0]\n",
    "            res.append(p.normal_form)\n",
    "    text = \" \".join(res)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция загрузка текста статьи с сайта РБК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(session, art_id):\n",
    "    url_id = art_id.replace(session, \"\")\n",
    "    url = \"https://www.rbc.ru/rbcfreenews/\"\n",
    "    full_url = url+url_id\n",
    "    try:\n",
    "        article =  requests.get(full_url)\n",
    "    except:\n",
    "        time.sleep(5.5)\n",
    "        article =  requests.get(full_url)\n",
    "    soup = BeautifulSoup(article.text)\n",
    "   \n",
    "    article_text =  soup.find('div', {'class': 'article__text article__text_free'}).findAll('p')\n",
    "    new_article = []\n",
    "    for i, elem in enumerate(article_text):\n",
    "        new_soup = BeautifulSoup(str(elem))\n",
    "        a_tags = new_soup.a\n",
    "        tags = []\n",
    "        tags_to_replace = []\n",
    "        if a_tags!=None:\n",
    "            \n",
    "            tags_to_replace =  [str(a_tags)]\n",
    "            tags = [a.string for a in a_tags]\n",
    "            if len(tags) !=0 and len(tags_to_replace) == len(tags):\n",
    "                for j, tag in enumerate(a_tags):\n",
    "                   \n",
    "                    new_article.append(str(new_soup).replace(str(tags_to_replace[j]),tags[j]).replace(u'\\xa0', u' ').replace(u'\\n', u' '))\n",
    "        else:\n",
    "            new_article.append(str(new_soup).replace(u'\\n', u' ').replace(u'\\xa0', u' '))\n",
    "    #print(new_article)\n",
    "    text = \"\".join([striphtml(res) for res in new_article])\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«»$]\", \" \", text)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполнение датафрейма текстом статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag['article'] = \"\"\n",
    "for index, row in tqdm(corpus_tag.iterrows()):\n",
    "    corpus_tag.loc[index, 'article'] = article_parser(row[\"session\"], index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.read_csv(\"train_dataset_train.csv\", index_col=0)\n",
    "test_df_to_append = pd.read_csv(\"test_dataset_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, corpus_tag[\"article\"], left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, corpus_tag[\"article\"], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка отсутствующих статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_parser(session, art_id):\n",
    "   \n",
    "    url_id = art_id.replace(session, \"\")\n",
    "    url = \"https://www.rbc.ru/rbcfreenews/\"\n",
    "    full_url = url+url_id\n",
    "    try:\n",
    "        article =  requests.get(full_url)\n",
    "    except:\n",
    "        time.sleep(5.5)\n",
    "        article =  requests.get(full_url)\n",
    "    soup = BeautifulSoup(article.text)\n",
    "   \n",
    "    article_text =  soup.find('div', {'class': 'article__text article__text_free'}).findAll('span')\n",
    "    new_article = []\n",
    "    for i, elem in enumerate(article_text):\n",
    "        new_soup = BeautifulSoup(str(elem))\n",
    "        a_tags = new_soup.a\n",
    "        tags = []\n",
    "        tags_to_replace = []\n",
    "        if a_tags!=None:\n",
    "            \n",
    "            tags_to_replace =  [str(a_tags)]\n",
    "            tags = [a.string for a in a_tags]\n",
    "            \n",
    "            if len(tags) !=0 and len(tags_to_replace) == len(tags):\n",
    "                for j, tag in enumerate(a_tags):\n",
    "                   \n",
    "                    new_article.append(str(new_soup).replace(str(tags_to_replace[j]),tags[j]).replace(u'\\xa0', u' ').replace(u'\\n', u' ')) \n",
    "        else:           \n",
    "            new_article.append(str(new_soup).replace(u'\\n', u' ').replace(u'\\xa0', u' '))      \n",
    "    text = \"\".join([striphtml(res) for res in new_article])\n",
    "    #text = text_cleaner(text)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|?€«»$]\", \" \", text)\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag = train_df_to_append[[\"article\",\"session\"]].append(test_df_to_append[[\"article\",\"session\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_corpus = corpus_tag[corpus_tag[\"article\"] == '']\n",
    "na_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_texts = []\n",
    "\n",
    "for index, row in tqdm(na_corpus.iterrows()):\n",
    "    corpus_tag.loc[index, 'article'] = span_parser(row[\"session\"], index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag[\"article\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append.drop(\"article\", axis=1, inplace=True)\n",
    "test_df_to_append.drop(\"article\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, corpus_tag[\"article\"], left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, corpus_tag[\"article\"], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсинг дополнительных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parser(session, art_id):\n",
    "   \n",
    "    url_id = art_id.replace(session, \"\")\n",
    "    url = \"https://www.rbc.ru/rbcfreenews/\"\n",
    "    full_url = url+url_id\n",
    "    try:\n",
    "        article =  requests.get(full_url)\n",
    "    except:\n",
    "        time.sleep(5.5)\n",
    "        article =  requests.get(full_url)\n",
    "    soup = BeautifulSoup(article.text)\n",
    "   \n",
    "    t =  soup.find('div', {'data-id': url_id})\n",
    "    meta_name = soup.find('meta',{'name':'news_keywords'})\n",
    "    meta_genre = soup.find('meta', {'itemprop':'genre'})\n",
    "    frame = pd.DataFrame(data={'data_type':t.attrs['data-type'],\n",
    "                         'categ':t.attrs['data-category-nick'],\n",
    "                         'aggregator':t.attrs['data-aggregator'],\n",
    "                         'char_len':t.attrs['data-chars-length'],\n",
    "                         'keyfeatures':meta_name[\"content\"],\n",
    "                         'genre':meta_genre[\"content\"]}, index=[art_id])\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_dataset_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_dataset_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag = train_df[[\"title\",\"session\"]].append(test_df[[\"title\",\"session\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.DataFrame(columns = [\"data_type\",\"categ\",\"aggregator\",\"char_len\",\"keyfeatures\",\"genre\"])\n",
    "for index, row in tqdm(corpus_tag.iterrows()):\n",
    "    df = df.append(data_parser(row[\"session\"], index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, df, left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсинг дополнительных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def meta_data_parser(session, art_id):\n",
    "    url_id = art_id.replace(session, \"\")\n",
    "    url = \"https://www.rbc.ru/rbcfreenews/\"\n",
    "    full_url = url+url_id\n",
    "    try:\n",
    "        article =  requests.get(full_url)\n",
    "    except:\n",
    "        time.sleep(5.5)\n",
    "        article =  requests.get(full_url)\n",
    "    soup = BeautifulSoup(article.text)\n",
    "   \n",
    "    t =  soup.find('a', {'class': \"article__header__category\", \"itemprop\":\"articleSection\"})\n",
    "    meta_address = soup.find('meta', {'itemprop':'address'})\n",
    "    header_cat = str(t.string)\n",
    "    meta_add = str(meta_address['content'])\n",
    "    del soup \n",
    "    del t\n",
    "    del meta_address\n",
    "    del article\n",
    "    return header_cat, meta_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_dataset_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_dataset_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag = train_df[[\"title\",\"session\"]].append(test_df[[\"title\",\"session\"]])\n",
    "del train_df\n",
    "del test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "header_cat_l = []\n",
    "meta_address_l = []\n",
    "\n",
    "index = corpus_tag.index.tolist()\n",
    "for index, row in tqdm(enumerate(index)):\n",
    "    header_cat, meta_address = meta_data_parser(corpus_tag.loc[row,\"session\"], row)\n",
    "    header_cat_l.append(header_cat)\n",
    "    meta_address_l.append(meta_address)\n",
    "    del header_cat\n",
    "    del meta_address\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag[\"header_cat\"] = header_cat_l\n",
    "corpus_tag[\"meta_address\"] = meta_address_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, corpus_tag[\"header_cat\"], left_index=True, right_index=True)\n",
    "train_df_to_append = pd.merge(train_df_to_append, corpus_tag[\"meta_address\"], left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, corpus_tag[\"header_cat\"], left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, corpus_tag[\"meta_address\"], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсинг данных об авторах статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authors_parser(session, art_id):\n",
    "    url_id = art_id.replace(session, \"\")\n",
    "    url = \"https://www.rbc.ru/rbcfreenews/\"\n",
    "    full_url = url+url_id\n",
    "    try:\n",
    "        article =  requests.get(full_url)\n",
    "    except:\n",
    "        time.sleep(5.5)\n",
    "        article =  requests.get(full_url)\n",
    "    soup = BeautifulSoup(article.text)\n",
    "   \n",
    "    t =  soup.find_all('span', {'class': \"article__authors__author__name\"})\n",
    "    \n",
    "    if len(t) == 0:\n",
    "        t =  str(soup.find('div', {'itemprop': \"author\"}).find('meta', {'itemprop':\"name\"})['content'])\n",
    "        return t\n",
    "    else:\n",
    "        authors = [str(author.string) for author in t]\n",
    "        return authors\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_dataset_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_dataset_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag = train_df[[\"title\",\"session\"]].append(test_df[[\"title\",\"session\"]])\n",
    "del train_df\n",
    "del test_df\n",
    "#np.log -> predict na predict np.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "authors=[]\n",
    "\n",
    "index = corpus_tag.index.tolist()\n",
    "for index, row in tqdm(enumerate(index)):\n",
    "    author = authors_parser(corpus_tag.loc[row,\"session\"], row)\n",
    "    if type(author) == list:\n",
    "        if len(author)>1:\n",
    "            author = ' '.join(author)\n",
    "        else:\n",
    "            author = author[0]\n",
    "    #print(author)\n",
    "    authors.append(author)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag[\"new_authors\"] = authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append = pd.merge(train_df_to_append, corpus_tag[\"new_authors\"], left_index=True, right_index=True)\n",
    "test_df_to_append = pd.merge(test_df_to_append, corpus_tag[\"new_authors\"], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_append.to_csv(\"full_train.csv\")\n",
    "test_df_to_append.to_csv(\"full_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
